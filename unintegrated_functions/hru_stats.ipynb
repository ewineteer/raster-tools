{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "from numba.core import types\n",
    "from numba.typed import Dict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insensitive_glob(pattern):\n",
    "    def either(c):\n",
    "        return '[%s%s]' % (c.lower(), c.upper()) if c.isalpha() else c\n",
    "    return glob.glob(''.join(map(either, pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_path = '/media/eric/data/box/Projects/Auckland/5. Model/LSPC/ZonalRasterStatistics/HRU_Sliders/Raster_Layers/'\n",
    "watersheds = [os.path.basename(f.path) for f in os.scandir(watershed_path) if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def rasterstats(combvals, elevals, elenodata, slopevals, slopenodata, vegvals, vegnodata):\n",
    "    \n",
    "    unique_vals = np.unique(combvals)\n",
    "    unique_vals = unique_vals[unique_vals!=combnodata]\n",
    "    \n",
    "    slope_dict = Dict.empty(key_type=types.int32,value_type=types.float64)\n",
    "    ele_dict = Dict.empty(key_type=types.int32,value_type=types.float64)\n",
    "    veg_dict = Dict.empty(key_type=types.int32,value_type=types.float64)\n",
    "\n",
    "    avgslope_dict = Dict.empty(key_type=types.int32,value_type=types.float64)\n",
    "    avgele_dict = Dict.empty(key_type=types.int32,value_type=types.float64)\n",
    "    avgveg_dict = Dict.empty(key_type=types.int32,value_type=types.float64)\n",
    "\n",
    "    slopecount_dict = Dict.empty(key_type=types.int32,value_type=types.int64)\n",
    "    elecount_dict = Dict.empty(key_type=types.int32,value_type=types.int64)\n",
    "    vegcount_dict = Dict.empty(key_type=types.int32,value_type=types.int64)\n",
    "\n",
    "    for val in unique_vals:\n",
    "        slope_dict[val] = 0\n",
    "        ele_dict[val] = 0\n",
    "        veg_dict[val] = 0\n",
    "\n",
    "        slopecount_dict[val] = 0\n",
    "        elecount_dict[val] = 0\n",
    "        vegcount_dict[val] = 0\n",
    "\n",
    "    xmax, ymax = combvals.shape\n",
    "    for x in range(0,xmax):\n",
    "        for y in range(0,ymax):\n",
    "            val = combvals[x,y]\n",
    "            if val != combnodata:\n",
    "                slopeval = slopevals[x,y]\n",
    "                vegval = vegvals[x,y]\n",
    "                eleval = elevals[x,y]\n",
    "\n",
    "                if slopeval != slopenodata:\n",
    "                    slope_dict[val] += slopeval\n",
    "                    slopecount_dict[val] += 1\n",
    "                if vegval != vegnodata:\n",
    "                    veg_dict[val] += vegval\n",
    "                    vegcount_dict[val] += 1\n",
    "                if eleval != elenodata:\n",
    "                    ele_dict[val] += eleval\n",
    "                    elecount_dict[val] += 1\n",
    "\n",
    "    for val in unique_vals:\n",
    "        if slopecount_dict[val]> 0 :\n",
    "            avgslope_dict[val] = slope_dict[val]/slopecount_dict[val]\n",
    "        if elecount_dict[val] > 0:\n",
    "            avgele_dict[val] = ele_dict[val]/elecount_dict[val]\n",
    "        if vegcount_dict[val] > 0:\n",
    "            avgveg_dict[val] = veg_dict[val]/vegcount_dict[val]\n",
    "    \n",
    "    return(avgslope_dict,avgele_dict,avgveg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_window_generator(hru_numrows,hru_numcols,blocksize):\n",
    "    colrange = range(0,hru_numcols,blocksize)\n",
    "    rowrange = range(0,hru_numrows,blocksize)\n",
    "    for col in colrange:\n",
    "        for row in rowrange:\n",
    "            topleft = [col, row]\n",
    "            \n",
    "            if col == max(colrange):\n",
    "                br_col = hru_numcols\n",
    "                block_width = hru_numcols - max(colrange)\n",
    "            else:\n",
    "                br_col = col + blocksize\n",
    "                block_width = blocksize\n",
    "            if row == max(rowrange):\n",
    "                br_row = hru_numrows\n",
    "                block_height = hru_numrows - max(rowrange)\n",
    "            else:\n",
    "                br_row = row + blocksize\n",
    "                block_height = blocksize\n",
    "                \n",
    "            bottomright = [br_col, br_row]\n",
    "            \n",
    "            col_offset = col\n",
    "            row_offset = row\n",
    "            \n",
    "            read_window =  Window(col_offset, row_offset, block_width, block_height)\n",
    "            \n",
    "            yield(read_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stolen directly from stackoverflow, dunno how it works\n",
    "def dictmean(dictlist):\n",
    "    sums = Counter()\n",
    "    counters = Counter()\n",
    "    for itemset in dictlist:\n",
    "        sums.update(itemset)\n",
    "        counters.update(itemset.keys())\n",
    "\n",
    "    ret = {x: float(sums[x])/counters[x] for x in sums.keys()}\n",
    "    return(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5166abe8e0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatersheds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwatersheds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "watersheds = [watersheds[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [04:11,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islands calculations complete.\n"
     ]
    }
   ],
   "source": [
    "for watershed in watersheds:\n",
    "    \n",
    "    elefile = insensitive_glob(f'/media/eric/data/data/auckland/hru_creation/nodata_interpolation/*levation_{watershed}.tif.tif')[0]\n",
    "    slopefile = insensitive_glob(f'/media/eric/data/data/auckland/hru_creation/nodata_interpolation/*lope_{watershed}.tif.tif')[0]\n",
    "    vegfile = insensitive_glob(f'/media/eric/data/data/auckland/hru_creation/nodata_interpolation/*egetation_{watershed}.tif.tif')[0]\n",
    "    combfile = insensitive_glob(f'/media/eric/data/box/Projects/Auckland/5. Model/LSPC/ZonalRasterStatistics/HRU_Sliders/Raster_Layers_Combine/{watershed}/*ombine.tif')[0]\n",
    "    combdbf = insensitive_glob(f'/media/eric/data/box/Projects/Auckland/5. Model/LSPC/ZonalRasterStatistics/HRU_Sliders/Raster_Layers_Combine/{watershed}/*ombine.tif.vat.dbf')[0]\n",
    "\n",
    "    if not os.path.exists(elefile):\n",
    "        print(f'ERROR IN FILESEARCH: {watershed}')\n",
    "    if not os.path.exists(slopefile):\n",
    "        print(f'ERROR IN FILESEARCH: {watershed}')\n",
    "    if not os.path.exists(vegfile):\n",
    "        print(f'ERROR IN FILESEARCH: {watershed}')\n",
    "    if not os.path.exists(combfile):\n",
    "        print(f'ERROR IN FILESEARCH: {watershed}')\n",
    "        \n",
    "    with rio.open(combfile) as src:\n",
    "        hru_numrows, hru_numcols = src.shape\n",
    "    \n",
    "    read_windows = read_window_generator(hru_numrows,hru_numcols,blocksize=4000)\n",
    "    \n",
    "    slopelist = []\n",
    "    elelist = []\n",
    "    veglist = []\n",
    "    \n",
    "    for read_window in tqdm(read_windows):\n",
    "\n",
    "        with rio.open(combfile) as src:\n",
    "            combvals = src.read(1, window=read_window)\n",
    "            combnodata = src.nodatavals[0]\n",
    "        with rio.open(elefile) as src:\n",
    "            elevals = src.read(1, window=read_window)\n",
    "            elenodata = src.nodatavals[0]\n",
    "        with rio.open(slopefile) as src:\n",
    "            slopevals = src.read(1, window=read_window)\n",
    "            slopenodata = src.nodatavals[0]\n",
    "        with rio.open(vegfile) as src:\n",
    "            vegvals = src.read(1, window=read_window)\n",
    "            vegnodata = src.nodatavals[0]\n",
    "\n",
    "        avgslope_dict,avgele_dict,avgveg_dict=rasterstats(combvals, elevals, elenodata, slopevals, slopenodata, vegvals, vegnodata)\n",
    "        slopelist.append(avgslope_dict)\n",
    "        elelist.append(avgele_dict)\n",
    "        veglist.append(avgveg_dict)\n",
    "        \n",
    "    avgslope_dict = dictmean(slopelist)\n",
    "    avgele_dict = dictmean(elelist)\n",
    "    avgveg_dict = dictmean(veglist)\n",
    "    \n",
    "    stats_table = pd.DataFrame([avgslope_dict,avgele_dict,avgveg_dict]).T\n",
    "    stats_table.columns = ['Avg_Slope','Avg_Elev','Avg_Veg']\n",
    "    stats_table.to_csv(f'/home/eric/projects/auckland/hru_creation/hru_stats/{watershed}_stats.csv')\n",
    "    print(f'{watershed} calculations complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insensitive_glob(f'/media/eric/data/data/auckland/hru_creation/nodata_interpolation/*levation_{watershed}.tif.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/eric/data/data/auckland/hru_creation/nodata_interpolation/elevation_islands.tif.tif']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insensitive_glob(f'/media/eric/data/data/auckland/hru_creation/nodata_interpolation/*levation_{watershed}.tif.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12344/12344 [1:59:58<00:00,  1.71it/s] \n"
     ]
    }
   ],
   "source": [
    "unique_vals, ele_avg, slope_avg, veg_avg = avg_noncategoricals(combvals, elevals, slopevals, vegvals)\n",
    "StatsDF = pd.DataFrame({'Comb_ID':unique_vals, 'Elevation':ele_avg,'Slope':slope_avg,'Vegetation':veg_avg})\n",
    "Combine_Table = gpd.read_file(combdbf)\n",
    "Combine_Table_With_Stats = Combine_Table.merge(right=StatsDF,how='inner',left_on='Value',right_on='Comb_ID')\n",
    "Combine_Table_With_Stats = Combine_Table_With_Stats.drop(columns=['geometry','Comb_ID'])\n",
    "Combine_Table_With_Stats.to_csv(f'/home/eric/projects/auckland/hru_creation/{watershed}_HRU_Table_With_Stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsDF = pd.DataFrame({'Comb_ID':unique_vals, 'Elevation':ele_avg,'Slope':slope_avg,'Vegetation':veg_avg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine_Table = gpd.read_file(combdbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine_Table_With_Stats = Combine_Table.merge(right=StatsDF,how='inner',left_on='Value',right_on='Comb_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine_Table_With_Stats = Combine_Table_With_Stats.drop(columns=['geometry','Comb_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine_Table_With_Stats.to_csv(f'/home/eric/projects/auckland/hru_creation/{watershed}_HRU_Table_With_Stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsDF.to_csv('/home/eric/projects/auckland/hru_creation/Tamaki_HRU_Table.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsDF = pd.read_csv('/home/eric/projects/auckland/hru_creation/Tamaki_HRU_Table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine_Table_With_Stats = Combine_Table.merge(right=StatsDF,how='inner',left_on='Value',right_on='Comb_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine_Table_With_Stats.to_csv('/home/eric/projects/auckland/hru_creation/Tamaki_HRU_Table_With_Stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dbfread'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-632fe242b832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdbfread\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDBF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dbfread'"
     ]
    }
   ],
   "source": [
    "from dbfread import DBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eric/miniconda3/envs/distgeo/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distgeo",
   "language": "python",
   "name": "distgeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
